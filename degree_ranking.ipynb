{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "degree_ranking.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoaI8wA5NTzZ",
        "colab_type": "text"
      },
      "source": [
        "##**Ranking nodes with degree centrality**\n",
        "\n",
        "In this Jupyter notebook instance, we simplify our model to single layer and test its effectiveness of approximating degree centrality ranking with varying graph structures. Results show that the model below can rank nodes based on degree centrality without actually getting any explicit information about degree in test datasets.\n",
        "\n",
        "\n",
        "Graphs used are undirected random graphs with 6 types of variants from networkx. 4 types are used for training and 2 types for testing. Though the ratio can be changed to see the effects. Number of epoches can be increased if the number of training graphs are being reduced.\n",
        "\n",
        "For choice of graphs, I just chose ones which can be generated by providing simply number of nodes, number of edges and minimum or no parameters.\n",
        "\n",
        "Note 1: The method doesn't work so well with Watts_Strogatz graphs, as you can check that there are many nodes with same degree values. The reason I think is that many nodes have same degree centrality values so model is not able to distinguish their ranking and it becomes harder to rank them.\n",
        "\n",
        "Note 2 : This Jupyter notebook was tested on Google Colaboratory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w7HQRBwXpWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import stuff\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from scipy.stats import kendalltau\n",
        "import random\n",
        "random.seed(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-Ey_op2YRmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_graph(num_nodes,num_edges,index):\n",
        "  \"\"\"Generate and return different types of graphs.\"\"\"\n",
        "  \n",
        "  if index==0:\n",
        "    g = nx.gnm_random_graph(num_nodes,num_edges)\n",
        "    return g\n",
        "  \n",
        "  elif index==1:\n",
        "    g = nx.barabasi_albert_graph(num_nodes,num_nodes-1)\n",
        "    return g\n",
        "  \n",
        "  elif index==2:\n",
        "    g = nx.powerlaw_cluster_graph(num_nodes,num_nodes-1,0.2)\n",
        "    return g\n",
        "  \n",
        "  elif index==3:\n",
        "    p = (num_edges/(num_nodes*num_nodes))\n",
        "    g = nx.fast_gnp_random_graph(num_nodes,p)\n",
        "    return g\n",
        "  \n",
        "  elif index==4:\n",
        "    #parameters taken from NetworkX example\n",
        "    g = nx.gaussian_random_partition_graph(100,10,10,.25,.1,directed=False)\n",
        "    return nx.Graph(g)\n",
        "  \n",
        "  elif index==5:\n",
        "    g = nx.watts_strogatz_graph(num_nodes,3,0.1)\n",
        "    return g\n",
        "  \n",
        "  \n",
        "def get_adjacency(list_graph,num_nodes):\n",
        "  \"\"\"Returns adjacency matrices, their node sequences and centrality matrix.\"\"\"\n",
        "  num_graph = len(list_graph)\n",
        "  list_adj = list()\n",
        "  cent_mat = np.zeros((num_nodes,num_graph),dtype=np.float)\n",
        "  list_nodelist = list()\n",
        "  \n",
        "  \n",
        "  for ind,g in enumerate(list_graph):\n",
        "    node_seq = list(g.nodes())\n",
        "    random.shuffle(node_seq)\n",
        "    degree_cent = nx.degree_centrality(g)\n",
        "    assert len(degree_cent)==100,\"Number of nodes do not match\"\n",
        "    \n",
        "    for i in range(num_nodes):\n",
        "      cent_mat[i,ind] = degree_cent[node_seq[i]]\n",
        "    \n",
        "    adj_mat = nx.adjacency_matrix(g,nodelist=node_seq)\n",
        "    adj_mat = sparse_mx_to_torch_sparse_tensor(adj_mat)\n",
        "    list_adj.append(adj_mat)\n",
        "    list_nodelist.append(node_seq)\n",
        "    \n",
        "       \n",
        "  return list_adj,list_nodelist,cent_mat\n",
        "      \n",
        "   \n",
        "def loss_cal(y_out,true_val,device):\n",
        "    \"\"\"Calculates ranking based loss function.\"\"\"\n",
        "    #Here we check the pairwise ranking of nodes. As including all pair of nodes\n",
        "    #is going to be computationally expensive, I chose to select N*20 number of node pairs.\n",
        "  \n",
        "    model_size = y_out.shape[0]\n",
        "    num_nodes = model_size\n",
        "\n",
        "    y_out = y_out.reshape((model_size))\n",
        "    true_val = true_val.reshape((model_size))\n",
        "    \n",
        "    _,order_y_true = torch.sort(-true_val[:num_nodes])\n",
        "\n",
        "    sample_num = num_nodes*20\n",
        "\n",
        "    ind_1 = torch.randint(0,num_nodes,(sample_num,)).long().to(device)\n",
        "    ind_2 = torch.randint(0,num_nodes,(sample_num,)).long().to(device)\n",
        "    \n",
        "\n",
        "    rank_measure=torch.sign(-1*(ind_1-ind_2)).float()\n",
        "        \n",
        "    input_arr1 = y_out[:num_nodes][order_y_true[ind_1]].to(device)\n",
        "    input_arr2 = y_out[:num_nodes][order_y_true[ind_2]].to(device)\n",
        "        \n",
        "\n",
        "    loss_rank = torch.nn.MarginRankingLoss(margin=1.0).forward(input_arr1,input_arr2,rank_measure)\n",
        " \n",
        "    return loss_rank\n",
        "\n",
        "\n",
        "def ranking_correlation(y_out,true_val):\n",
        "    \"\"\"Returns Kendall-Tau's correlation and top-k accuracy.\"\"\"\n",
        "    model_size = y_out.shape[0]\n",
        "    node_num = model_size\n",
        "    y_out = y_out.reshape((model_size))\n",
        "    true_val = true_val.reshape((model_size))\n",
        "\n",
        "    predict_arr = y_out.cpu().detach().numpy()\n",
        "    true_arr = true_val.cpu().detach().numpy()\n",
        "    \n",
        "    #check top-k accuracy\n",
        "    k = 20\n",
        "    top_k_predict = predict_arr.argsort()[::-1][:k]\n",
        "    top_k_true = true_arr.argsort()[::-1][:k]\n",
        "    \n",
        "    acc = 0\n",
        "    for item in top_k_predict:\n",
        "      if item in top_k_true:\n",
        "        acc += 1\n",
        "        \n",
        "    kt,_ = kendalltau(predict_arr[:node_num],true_arr[:node_num])\n",
        "\n",
        "    return kt, (acc/k)*100,k\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    \n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "  \n",
        "  \n",
        "    \n",
        "def train(list_adj,cent_mat,device):\n",
        "  \"\"\"Training the model.\"\"\"\n",
        "  model.train()\n",
        "  num_train = len(list_adj)\n",
        "  for i in range(num_train):\n",
        "    adj = list_adj[i]\n",
        "    adj = adj.to(device)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    y_out = model(adj)\n",
        "    true_val = torch.from_numpy(cent_mat[:,i]).float()\n",
        "    true_val = true_val.to(device)\n",
        "    \n",
        "    loss_rank = loss_cal(y_out,true_val,device)\n",
        "    loss_rank.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "def test(list_adj,cent_mat,device):\n",
        "  \"\"\"Testing the model.\"\"\"\n",
        "  model.eval()\n",
        "  list_kt = list()\n",
        "  list_acc = list()\n",
        "  num_test = len(list_adj)\n",
        "  for j in range(num_test):\n",
        "    adj = list_adj[j]\n",
        "    adj = adj.to(device)\n",
        "    \n",
        "    y_out = model(adj)\n",
        "    \n",
        "    true_val = torch.from_numpy(cent_mat[:,j]).float()\n",
        "    true_val = true_val.to(device)\n",
        "    \n",
        "    kt,acc,k = ranking_correlation(y_out,true_val)\n",
        "    list_kt.append(kt)\n",
        "    list_acc.append(acc)\n",
        "    \n",
        "  print(f\"Average Kendall Tau score is: {np.mean(np.array(list_kt))} and std:{np.std(np.array(list_kt))}\")\n",
        "  print(f\"Average top-{k} accuracy:{np.mean(list_acc)}% and std:{np.std(list_acc)}\")\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-WVdIV7dEf4",
        "colab_type": "code",
        "outputId": "abfc4d74-f23b-42cf-af29-d1efbb7bef4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#generate the indices for graph generation\n",
        "#select all graphs and train/test on different graphs\n",
        "graph_index = list(range(6))\n",
        "\n",
        "#To generate graphs of same type,comment the above line and uncomment the line below\n",
        "# index in bracket[] decides which graph to choose\n",
        "#graph_index = [4]*6\n",
        "\n",
        "#shuffle the indices for random training and testing dataset\n",
        "#Running this every time will create different combinations\n",
        "\n",
        "random.shuffle(graph_index)\n",
        "\n",
        "#choose 4 graph variants for training and 2 for testing\n",
        "train_index = graph_index[:4]\n",
        "test_index = graph_index[4:]\n",
        "\n",
        "#create list of graphs for training\n",
        "#below numbers are chosen randomly without much thought\n",
        "num_nodes = 100\n",
        "num_edges = 400\n",
        "\n",
        "list_train_graph = list()\n",
        "list_test_graph = list()\n",
        "\n",
        "#create list for training graphs\n",
        "print(\"Create training graphs\")\n",
        "for i in range(50):\n",
        "  for j in train_index:\n",
        "    new_graph = get_graph(num_nodes,num_edges,j)\n",
        "    \n",
        "    #making sure graphs were generated as we wanted\n",
        "    assert nx.is_directed(new_graph)==False,\"Graph is directed\"\n",
        "    assert nx.number_of_selfloops(new_graph)==0,\"Graph has self-loops\"\n",
        "    assert new_graph.number_of_nodes() == num_nodes,\"Number of nodes are higher\"\n",
        "    \n",
        "    list_train_graph.append(new_graph)\n",
        "    \n",
        "#create list for test graphs\n",
        "print(\"Create testing graphs\")\n",
        "for i in range(5):\n",
        "  for j in test_index:\n",
        "    new_graph = get_graph(num_nodes,num_edges,j)\n",
        "    \n",
        "    #making sure graphs were generated as we wanted\n",
        "    assert nx.is_directed(new_graph)==False,\"Graph is directed\"\n",
        "    assert nx.number_of_selfloops(new_graph)==0,\"Graph has self-loops\"\n",
        "    list_test_graph.append(new_graph)\n",
        "\n",
        "#shuffle the graphs\n",
        "random.shuffle(list_train_graph)\n",
        "random.shuffle(list_test_graph)\n",
        "\n",
        "\n",
        "#get training and testing data\n",
        "\n",
        "list_adj_train,list_nodelist_train,cent_mat_train = get_adjacency(list_train_graph,num_nodes)\n",
        "list_adj_test,list_nodelist_test,cent_mat_test = get_adjacency(list_test_graph,num_nodes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create training graphs\n",
            "Create testing graphs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smv5suGwk0zU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define simple MLP module\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size,hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "input_size = num_nodes\n",
        "hidden_size = 16\n",
        "output_size = 1\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juQj_iBepmFq",
        "colab_type": "code",
        "outputId": "3d1f7900-9895-4968-bcdd-67f9b20c7b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "#Training the model \n",
        "#To check for different combinations of graphs its better to just click on \n",
        "# \"Restart and Run all\" in Runtime menu.\n",
        "\n",
        "num_epoch = 10\n",
        "\n",
        "for e in range(num_epoch):\n",
        "  print(\"--------------------------------------------------------\")\n",
        "  print(f\"Epoch number: {e}\")\n",
        "  train(list_adj_train,cent_mat_train,device)\n",
        "  test(list_adj_test,cent_mat_test,device)\n",
        "  \n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------\n",
            "Epoch number: 0\n",
            "Average Kendall Tau score is: 0.7853640460813041 and std:0.02905096230478589\n",
            "Average top-20 accuracy:75.0% and std:7.0710678118654755\n",
            "--------------------------------------------------------\n",
            "Epoch number: 1\n",
            "Average Kendall Tau score is: 0.9193699863927408 and std:0.00925319424462081\n",
            "Average top-20 accuracy:87.5% and std:3.3541019662496847\n",
            "--------------------------------------------------------\n",
            "Epoch number: 2\n",
            "Average Kendall Tau score is: 0.9534167747306178 and std:0.006485969252174808\n",
            "Average top-20 accuracy:92.5% and std:4.6097722286464435\n",
            "--------------------------------------------------------\n",
            "Epoch number: 3\n",
            "Average Kendall Tau score is: 0.9559727914090107 and std:0.008695863686838983\n",
            "Average top-20 accuracy:93.0% and std:4.58257569495584\n",
            "--------------------------------------------------------\n",
            "Epoch number: 4\n",
            "Average Kendall Tau score is: 0.955931059485191 and std:0.00863798621454829\n",
            "Average top-20 accuracy:91.5% and std:5.024937810560445\n",
            "--------------------------------------------------------\n",
            "Epoch number: 5\n",
            "Average Kendall Tau score is: 0.955931059485191 and std:0.00863798621454829\n",
            "Average top-20 accuracy:93.0% and std:4.0\n",
            "--------------------------------------------------------\n",
            "Epoch number: 6\n",
            "Average Kendall Tau score is: 0.955721564067678 and std:0.00844301267715444\n",
            "Average top-20 accuracy:92.5% and std:4.031128874149275\n",
            "--------------------------------------------------------\n",
            "Epoch number: 7\n",
            "Average Kendall Tau score is: 0.9553862920783682 and std:0.008135860605154166\n",
            "Average top-20 accuracy:92.5% and std:4.6097722286464435\n",
            "--------------------------------------------------------\n",
            "Epoch number: 8\n",
            "Average Kendall Tau score is: 0.9542508121315534 and std:0.007615584599958597\n",
            "Average top-20 accuracy:91.0% and std:5.385164807134504\n",
            "--------------------------------------------------------\n",
            "Epoch number: 9\n",
            "Average Kendall Tau score is: 0.9537889584116988 and std:0.007292030135085588\n",
            "Average top-20 accuracy:92.5% and std:4.6097722286464435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuVMHS8cIhNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}